{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ylmHRNZuexnE"
      },
      "outputs": [],
      "source": [
        "# install pytorch lithening\n",
        "!pip install pytorch-lightning --quiet\n",
        "!pip install wandb -Uq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5dPxEJZwe6Y2"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torchmetrics import Accuracy\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CczgZ5nkgAGy"
      },
      "outputs": [],
      "source": [
        "# create one class to deal with data\n",
        "class CifarDataModule(pl.LightningDataModule):\n",
        "  def __init__(self, batch_size, data_dir=\"./\"):\n",
        "    super().__init__()\n",
        "    self.data_dir=data_dir\n",
        "    self.batch_size=batch_size\n",
        "    self.transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "    self.num_classes=10\n",
        "\n",
        "  def prepare_data(self):\n",
        "    CIFAR10(self.data_dir,train=True,download=True)\n",
        "    CIFAR10(self.data_dir,train=False,download=True)\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    if stage=='fit' or stage is None:\n",
        "      cifar_full=CIFAR10(self.data_dir,train=True,transform=self.transform)\n",
        "      self.cifar_train,self.cifar_val=random_split(cifar_full,[45000,5000])\n",
        "\n",
        "    if stage=='test' or stage is None:\n",
        "      self.cifar_test=CIFAR10(self.data_dir,train=False,transform=self.transform)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.cifar_train,batch_size=self.batch_size,shuffle=True,num_workers=2)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.cifar_val,batch_size=self.batch_size,shuffle=False,num_workers=2)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.cifar_test,batch_size=self.batch_size,shuffle=False,num_workers=2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-Xal7kSzlkAb"
      },
      "outputs": [],
      "source": [
        "class CIFAR10LitModel(pl.LightningModule):\n",
        "    def __init__(self, input_shape,num_classes,config,learning_rate=3e-4):\n",
        "      super().__init__()\n",
        "      self.save_hyperparameters()\n",
        "      self.input_shape=input_shape\n",
        "      self.learning_rate=learning_rate\n",
        "      self.activation=F.relu\n",
        "\n",
        "      # model architecture\n",
        "      self.conv1=nn.Conv2d(3,32,3,1)\n",
        "      self.conv2=nn.Conv2d(32,32,3,1)\n",
        "      self.conv3=nn.Conv2d(32,64,3,1)\n",
        "      self.conv4=nn.Conv2d(64,64,3,1)\n",
        "      self.pool1=nn.MaxPool2d(2)\n",
        "      self.pool2=nn.MaxPool2d(2)\n",
        "\n",
        "      n_sizes = self._get_output_shape(input_shape)\n",
        "      # Edited by Indra Narayan Dutta: Number of neurons now added as hyperparameter using sweep\n",
        "      fc1_neurons = config.fc1;\n",
        "      fc2_neurons = config.fc2;\n",
        "      fc3_neurons = config.fc3;\n",
        "      fc4_neurons = config.fc4;\n",
        "      self.fc1=nn.Linear(n_sizes,fc1_neurons)\n",
        "      self.fc2=nn.Linear(fc1_neurons,fc2_neurons)\n",
        "      self.fc3=nn.Linear(fc2_neurons,fc3_neurons)\n",
        "      self.fc4=nn.Linear(fc3_neurons,num_classes)\n",
        "\n",
        "      self.train_acc=Accuracy(task='multiclass',num_classes=10)\n",
        "      self.val_acc=Accuracy(task='multiclass',num_classes=10)\n",
        "      self.test_acc=Accuracy(task='multiclass',num_classes=10)\n",
        "\n",
        "\n",
        "    def _get_output_shape(self, shape):\n",
        "          '''returns the size of the output tensor from the conv layers'''\n",
        "          batch_size = 1\n",
        "          input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
        "          output_feat = self._feature_extractor(input)\n",
        "          n_size = output_feat.data.view(batch_size, -1).size(1)\n",
        "          return n_size\n",
        "\n",
        "\n",
        "  # conv1,relu, conv2,relu, maxpool,conv3,relu,conv4,relu,maxpool\n",
        "    def _feature_extractor(self,x):\n",
        "      x=self.activation(self.conv1(x))\n",
        "      x=self.pool1(F.relu(self.conv2(x)))\n",
        "      x=self.activation(self.conv3(x))\n",
        "      x=self.pool2(F.relu(self.conv4(x)))\n",
        "      return x\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "      x=self._feature_extractor(x)\n",
        "      x=x.view(x.size(0),-1)\n",
        "      x=self.activation(self.fc1(x))\n",
        "      x=self.activation(self.fc2(x))                                  #Indra Narayan Dutta: new fc2 layer of 256 added to forward pass\n",
        "      x=self.activation(self.fc3(x))\n",
        "      x=F.log_softmax(self.fc4(x),dim=1)\n",
        "      return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      x, y = batch\n",
        "      logits = self(x)\n",
        "      loss = F.nll_loss(logits, y)\n",
        "      # metric\n",
        "      preds = torch.argmax(logits, dim=1)\n",
        "      acc = self.train_acc(preds, y)\n",
        "      self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
        "      self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
        "      return loss\n",
        "\n",
        "    # validation loop\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "      x, y = batch\n",
        "      logits = self(x)\n",
        "      loss = F.nll_loss(logits, y)\n",
        "      preds = torch.argmax(logits, dim=1)\n",
        "      acc = self.val_acc(preds, y)\n",
        "      self.log('val_loss', loss, prog_bar=True)\n",
        "      self.log('val_acc', acc, prog_bar=True)\n",
        "      return loss\n",
        "\n",
        "    # test loop\n",
        "    def test_step(self,batch,batch_idx):\n",
        "      x,y=batch\n",
        "      logits=self(x)\n",
        "      loss=F.nll_loss(logits,y)\n",
        "\n",
        "      pred=torch.argmax(logits,dim=1)\n",
        "      acc=self.test_acc(pred,y)\n",
        "      self.log('test_loss',loss,on_epoch=True)\n",
        "      self.log('test_acc',acc,on_epoch=True)\n",
        "      return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "      optimizer=torch.optim.Adam(self.parameters(),self.learning_rate)\n",
        "      return optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IRAcw-MZYebS"
      },
      "outputs": [],
      "source": [
        "# class for visualizing one batch of validation images along with predicted and rall class label\n",
        "class ImagePredictionLogger(pl.Callback):\n",
        "    def __init__(self, val_samples, num_samples=32):\n",
        "        super().__init__()\n",
        "        self.val_imgs, self.val_labels = val_samples\n",
        "        self.val_imgs = self.val_imgs[:num_samples]\n",
        "        self.val_labels = self.val_labels[:num_samples]\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
        "        logits = pl_module(val_imgs)\n",
        "        preds = torch.argmax(logits, 1)\n",
        "\n",
        "        trainer.logger.experiment.log({\n",
        "            \"examples\": [wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\")\n",
        "                            for x, pred, y in zip(val_imgs, preds, self.val_labels)],\n",
        "            \"global_step\": trainer.global_step\n",
        "            })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7kOEzHpmCYi",
        "outputId": "8c449eaf-857b-45fa-d1f4-3da8b425f6ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "cifar = CifarDataModule(batch_size=32, data_dir=\"./\")\n",
        "cifar.prepare_data()\n",
        "cifar.setup()\n",
        "# grab samples to log predictions on\n",
        "samples = next(iter(cifar.val_dataloader()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pbTicSoZ1Gtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa85541-870c-4fcc-c0f5-e8d50b206aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 1xjzw1l3\n",
            "Sweep URL: https://wandb.ai/msc_bme/lastt/sweeps/1xjzw1l3\n"
          ]
        }
      ],
      "source": [
        "from wandb.env import CONFIG_DIR\n",
        "### WandB, you have have an account(if you don't, create one)\n",
        "wandb.login(key='')\n",
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "    }\n",
        "metric = {\n",
        "    'name': 'loss',\n",
        "    'goal': 'minimize'\n",
        "    }\n",
        "sweep_config['metric'] = metric\n",
        "parameters_dict = {\n",
        "    'fc1': {\n",
        "        'values': [128, 256, 512]\n",
        "        },\n",
        "    'fc2': {\n",
        "        'values': [128, 256, 512]\n",
        "        },\n",
        "    'fc3': {\n",
        "        'values': [128, 256, 512]\n",
        "        },\n",
        "    'fc4': {\n",
        "        'values': [128, 256, 512]\n",
        "        },\n",
        "    'dropout': {\n",
        "          'values': [0.3, 0.4, 0.5]\n",
        "        },\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "parameters_dict.update({\n",
        "    'epochs': {\n",
        "        'value': 1}\n",
        "    })\n",
        "parameters_dict.update({\n",
        "    'batch_size': {\n",
        "        # integers between 32 and 256\n",
        "        # with evenly-distributed logarithms\n",
        "        'distribution': 'q_log_uniform_values',\n",
        "        'q': 8,\n",
        "        'min': 32,\n",
        "        'max': 256,\n",
        "      }\n",
        "    })\n",
        "\n",
        "wandb_logger = WandbLogger(project='lastt', job_type='train', log_model=\"all\")\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project=\"lastt\")\n",
        "def train_model(learning_rate=1e-3):\n",
        "\n",
        "    config=wandb.config\n",
        "\n",
        "\n",
        "    with wandb.init(config=config):\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "        # instantiate classes\n",
        "        dm = CifarDataModule(config.batch_size)\n",
        "        dm.prepare_data()\n",
        "        dm.setup()\n",
        "        model = CIFAR10LitModel((3, 32, 32), dm.num_classes, config=config)\n",
        "        wandb_logger.watch(model)\n",
        "        # Initialize Callbacks\n",
        "        checkpoint_callback = pl.callbacks.ModelCheckpoint()\n",
        "        early_stop_callback = pl.callbacks.EarlyStopping(monitor=\"val_acc\", patience=3, verbose=False, mode=\"max\")\n",
        "        ### WandB\n",
        "        trainer = pl.Trainer(max_epochs=5,\n",
        "                     logger=wandb_logger,\n",
        "                     callbacks=[checkpoint_callback, early_stop_callback,ImagePredictionLogger(samples)]\n",
        "                    )\n",
        "      # Train the model\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluate the model\n",
        "    trainer.test(dataloaders=cifar.test_dataloader())\n",
        "    # tell the WandB you have finished\n",
        "    wandb.finish()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "edXZVgxLd7_b",
        "outputId": "8d3efaa8-61c0-429e-d9e3-c700433d1d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yn6ioku0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc1: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc2: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc3: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc4: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "wandb.agent(sweep_id, function=train_model, count=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}